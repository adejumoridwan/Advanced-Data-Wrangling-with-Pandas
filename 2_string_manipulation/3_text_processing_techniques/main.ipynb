{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing Techniques using Pandas\n",
    "\n",
    "Text preprocessing is a crucial step in natural language processing (NLP) tasks, as it helps to clean and transform raw text data into a format that can be more effectively analyzed and understood by machine learning models. In this lecture, we will explore three key text preprocessing techniques using the pandas library in Python: tokenization, stop word removal, and stemming/lemmatization.\n",
    "\n",
    "## Tokenization\n",
    "\n",
    "Tokenization is the process of breaking down a given text into smaller units, called tokens. These tokens can be individual words, phrases, or even characters, depending on the specific task at hand. In pandas, we can use the `str.split()` method to tokenize text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# Load sample text data from the 20 Newsgroups dataset\n",
    "data = fetch_20newsgroups(subset='all')\n",
    "df = pd.DataFrame({'text': data.data})\n",
    "\n",
    "# Tokenize the text data\n",
    "df['tokens'] = df['text'].str.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Word Removal\n",
    "\n",
    "Stop words are common words that do not carry significant meaning in the context of text analysis, such as \"the\", \"a\", \"and\", \"is\", etc. Removing stop words can help to focus on the more meaningful words in the text and improve the performance of NLP models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Word Removal\n",
    "\n",
    "Stop words are common words that do not carry significant meaning in the context of text analysis, such as \"the\", \"a\", \"and\", \"is\", etc. Removing stop words can help to focus on the more meaningful words in the text and improve the performance of NLP models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "# Remove stop words from the tokenized text\n",
    "df['filtered_tokens'] = df['tokens'].apply(lambda x: [word for word in x if word.lower() not in ENGLISH_STOP_WORDS])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming and Lemmatization\n",
    "\n",
    "Stemming and lemmatization are techniques used to reduce words to their base or root form, known as the stem or lemma, respectively. Stemming typically involves removing common suffixes, while lemmatization takes into account the context and part of speech of the word to determine its base form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'load_reuters' from 'sklearn.datasets' (c:\\Users\\abu aisha\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\datasets\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_reuters\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtextblob\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TextBlob\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Load the Reuters text dataset\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'load_reuters' from 'sklearn.datasets' (c:\\Users\\abu aisha\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\datasets\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# Stemming\n",
    "stemmer = PorterStemmer()\n",
    "df['stemmed_tokens'] = df['filtered_tokens'].apply(lambda x: [stemmer.stem(word) for word in x])\n",
    "\n",
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "df['lemmatized_tokens'] = df['filtered_tokens'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
